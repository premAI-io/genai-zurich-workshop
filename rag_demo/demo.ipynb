{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9165d7f7-b2fe-418b-84f8-4de68cc3475d",
   "metadata": {},
   "source": [
    "## Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260c1eeb-8d86-473a-85f6-eeeda115696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/rag_demo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f333c-d331-4dc6-8e9d-e9e2ac177442",
   "metadata": {},
   "source": [
    "## Define the global constants\n",
    "\n",
    "Note that this runs completely offline.\n",
    "1. We use the `Alibaba-NLP/gte-large-en-v1.5` model as the embedding model from HuggingFace.\n",
    "2. We use the `premai-io/prem-1B-chat` for the chat model instead of OpenAI (GPT 3.5 or GPT 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ff71bf-d496-40ac-9f54-9efc2d167420",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "EMBED_MODEL_NAME = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "CHAT_MODEL_NAME = 'premai-io/prem-1B-chat'\n",
    "\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 40\n",
    "\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the users's question. If you can't find the answer in the context, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context: ```\n",
    "{context}\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd97551-bcad-411c-8704-05eeb8fb44ba",
   "metadata": {},
   "source": [
    "## Embed the docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f9221-5673-4208-bb7d-aefe1a390ff2",
   "metadata": {},
   "source": [
    "#### Load the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063ea61b-f9ad-4db9-b2eb-6dbbe69ec465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:37<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for f in tqdm(list(Path('docs').iterdir())):\n",
    "    if f.suffix == '.pdf':\n",
    "        loader = PyPDFLoader(f)\n",
    "        docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f38e37-aba0-42fc-ac7d-befb111b3beb",
   "metadata": {},
   "source": [
    "#### Split the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d024adff-1511-4d34-88ca-229b71dbd112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL_NAME)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9dba8d-a9fb-4649-83f6-51c91cff196f",
   "metadata": {},
   "source": [
    "#### Create the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7720b630-4e57-4605-9067-84f8f8ee0782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/rag_demo/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = {'device': DEVICE, 'trust_remote_code': True}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embedding_gen = HuggingFaceEmbeddings(\n",
    "    model_name=EMBED_MODEL_NAME,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f014f43-20f7-4cb7-adb2-d5a50cc13dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "def generate_result(model, tokenizer, messages, terminators):\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Generate\n",
    "    inputs = tokenizer(prompt, return_attention_mask=False, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids']\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    res = model.generate(input_ids=input_ids, max_new_tokens=400, pad_token_id=tokenizer.pad_token_id, eos_token_id=terminators)\n",
    "    generated_text = tokenizer.decode(res[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def ask_question(ques, vectorstore, model, tokenizer, terminators):\n",
    "    docs = vectorstore.similarity_search(ques, k=7)\n",
    "    relevant_context = format_docs(docs)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", RAG_SYSTEM_PROMPT), (\"user\", \"{input}\")]\n",
    "    )\n",
    "    messages = prompt.invoke({'context': relevant_context, 'input': 'hi'}).messages\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': messages[0].content},\n",
    "        {'role': 'user', 'content': messages[1].content},\n",
    "    ]\n",
    "    res = generate_result(model, tokenizer, messages, terminators)\n",
    "    return {'response': res, 'relevant_docs': [d.page_content for d in docs]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006f7e7-fd31-49c4-accb-75386cdd6f70",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f43fe65-36ac-4fcb-92af-61f99fae5e11",
   "metadata": {},
   "source": [
    "#### Load the `premai-io/prem-1B-chat` model from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae8adaf3-815b-41ba-8f5f-e3dc22a4d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained('premai-io/prem-1B-chat', torch_dtype=torch.bfloat16)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Setup terminators\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.encode('<|eot_id|>', add_special_tokens=False)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a5e17-74e4-4e12-8e04-60b12c59d7f9",
   "metadata": {},
   "source": [
    "#### Ask the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "995b7291-dba3-4618-9675-31e4a639859c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main establishment of a data intermediation services provider in the Union should be the place of its central administration in the Union.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = ask_question('What is the head of union?', vectorstore, model, tokenizer, terminators)\n",
    "res['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5a6c0-1152-43cd-9112-684bf44f08b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52264fab-5a49-4e8f-ba83-7d38e8d694d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1f4ff-35fd-4a04-9ea2-8088a893135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/premAI-io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb0d84b-3de8-4b77-836e-ab77374019bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://rohitgr7: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@github.com/premAI-io/genai-zurich-workshop.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba78d8-a495-4eaa-b43c-0b5d9fe6e839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5ce5b-d3ba-4ae9-abe7-8ed291bf7e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70597901-ac08-4b30-a62c-c102056280dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea16995-de55-46d9-80ea-515f6549f046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
