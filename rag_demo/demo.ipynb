{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9165d7f7-b2fe-418b-84f8-4de68cc3475d",
   "metadata": {},
   "source": [
    "## Load the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260c1eeb-8d86-473a-85f6-eeeda115696c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/rag_demo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f333c-d331-4dc6-8e9d-e9e2ac177442",
   "metadata": {},
   "source": [
    "## Define the global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ff71bf-d496-40ac-9f54-9efc2d167420",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "CHAT_MODEL_NAME = 'premai-io/prem-1B-chat'\n",
    "\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 50\n",
    "TOP_K_DOCS = 8\n",
    "MIN_SCORE=0.3\n",
    "\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are a helpful assistant, optimized for RAG. Please answer the user question, based on the given context. But avoid sentences like 'based on the given context' in the response.\n",
    "If you can't find the answer in the context, just say that you don't know. Keep the response short. Don't quote anything from the context directly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd97551-bcad-411c-8704-05eeb8fb44ba",
   "metadata": {},
   "source": [
    "## Embed the docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f9221-5673-4208-bb7d-aefe1a390ff2",
   "metadata": {},
   "source": [
    "#### Load the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063ea61b-f9ad-4db9-b2eb-6dbbe69ec465",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14/14 [00:11<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for f in tqdm(list(Path('docs/papers').iterdir())):\n",
    "    if f.suffix == '.pdf':\n",
    "        loader = PyPDFLoader(f)\n",
    "        docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f38e37-aba0-42fc-ac7d-befb111b3beb",
   "metadata": {},
   "source": [
    "#### Split the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d024adff-1511-4d34-88ca-229b71dbd112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL_NAME)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(tokenizer, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9dba8d-a9fb-4649-83f6-51c91cff196f",
   "metadata": {},
   "source": [
    "#### Create the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7720b630-4e57-4605-9067-84f8f8ee0782",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_gen = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    splits,\n",
    "    embedding_gen,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f014f43-20f7-4cb7-adb2-d5a50cc13dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "def generate_result(model, tokenizer, messages, terminators):\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Generate\n",
    "    inputs = tokenizer(prompt, return_attention_mask=False, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids']\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    res = model.generate(input_ids=input_ids, max_new_tokens=400, pad_token_id=tokenizer.pad_token_id, eos_token_id=terminators, do_sample=False)\n",
    "    generated_text = tokenizer.decode(res[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def ask_question(ques, vectorstore, model, tokenizer, terminators):\n",
    "    docs_with_scores = vectorstore.similarity_search_with_score(ques, k=TOP_K_DOCS)\n",
    "    docs = [doc for doc, score in docs_with_scores if score > MIN_SCORE]\n",
    "    if not docs:\n",
    "        raise Exception('No docs found')\n",
    "    relevant_context = format_docs(docs)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", RAG_SYSTEM_PROMPT), (\"user\", \"{input}\")]\n",
    "    )\n",
    "    messages = prompt.invoke({'context': relevant_context, 'input': 'hi'}).messages\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': messages[0].content},\n",
    "        {'role': 'user', 'content': messages[1].content},\n",
    "    ]\n",
    "    res = generate_result(model, tokenizer, messages, terminators)\n",
    "    return {'response': res, 'relevant_docs': [d.page_content for d in docs]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006f7e7-fd31-49c4-accb-75386cdd6f70",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f43fe65-36ac-4fcb-92af-61f99fae5e11",
   "metadata": {},
   "source": [
    "#### Load the `premai-io/prem-1B-chat` model from Huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae8adaf3-815b-41ba-8f5f-e3dc22a4d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained('premai-io/prem-1B-chat', torch_dtype=torch.bfloat16)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Setup terminators\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.encode('<|eot_id|>', add_special_tokens=False)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a5e17-74e4-4e12-8e04-60b12c59d7f9",
   "metadata": {},
   "source": [
    "#### Ask the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "620af965-3708-4484-b064-c42982f7110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the key feature of ChatEval compared to the other evaluation strategies? Give a short answer.\",\n",
    "    \"How does the Infini-attention technique aim to address the problem related to limited context in generative models? Give a short answer.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d716a62d-4072-440d-a937-9280424d09cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mWhat is the key feature of ChatEval compared to the other evaluation strategies? Give a short answer.\u001b[0m\n",
      "\u001b[96mChatEval is a multi-agent evaluation framework that employs a LLM-based approach to evaluate text. The framework is designed to enable human-like evaluation of text by leveraging the strengths of different LLMs. The framework consists of a single-agent model, which is responsible for generating responses to the evaluation task, and a multi-agent model, which is responsible for collaborating with the single-agent model to generate responses. The framework is designed to be scalable and flexible, allowing for the evaluation of text from different perspectives and with different levels of human expertise.\n",
      "\n",
      "In this paper, we present the design of ChatEval and discuss the key components of the framework. We also discuss the effect of different communication strategies on the evaluation performance of ChatEval. We conclude by discussing the future of ChatEval and its potential applications in natural language processing.\n",
      "\n",
      "Key components of ChatEval\n",
      "\n",
      "1. Single-agent model: The single-agent model is responsible for generating responses to the evaluation task. It consists of a single agent, which is responsible for generating responses to the evaluation task. The agent is responsible for generating responses to the evaluation task in a collaborative manner, using the LLMs to generate responses.\n",
      "\n",
      "2. Multi-agent model: The multi-agent model is responsible for collaborating with the single-agent model to generate responses. It consists of a plurality of agents, each of which is responsible for generating responses to the evaluation task. The plurality of agents is responsible for generating responses to the evaluation task in a collaborative manner, using the LLMs to generate responses.\n",
      "\n",
      "3. Collaborative communication: The framework employs a collaborative communication mechanism to enable the evaluation of text. The framework uses a chatbot to communicate with the single-agent model and the multi-agent model. The chatbot is responsible\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[94mHow does the Infini-attention technique aim to address the problem related to limited context in generative models? Give a short answer.\u001b[0m\n",
      "\u001b[96mThe Infini-attention is a new approach to scale Transformer-based Large Language Models (LLMs) to a vast range of contexts. It is based on the idea of compressive memory and local attention, which enables efficient computation of long-term contexts. The approach is efficient in terms of memory footprint and computation time, and it is scalable to a large number of input sequences. The approach is also scalable to a large number of contexts, and it is able to handle long-term contexts with a bounded memory footprint. The approach is also able to handle long-term contexts with a bounded memory footprint, and it is able to handle long-term contexts with a bounded memory footprint. The approach is also able to handle long-term contexts with a bounded memory footprint, and it is able to handle long-term contexts with a bounded memory footprint. The approach is also able to handle long-term contexts with a bounded memory footprint, and it is able to handle long-term contexts with a bounded memory footprint. The approach is also able to handle long-term contexts with a bounded memory footprint, and it is able to handle long-term contexts with a bounded memory footprint. The approach is also able to handle long-term contexts with a bounded memory footprint, and it is able to handle long-term contexts with a bounded memory footprint. The approach is also able to handle long-term contexts with a bounded memory footprint, and it is able to handle long-term contexts with a bounded memory footprint. The approach is also able to handle long-term contexts with a bounded memory footprint, and it is able to handle long-term contexts with a bounded memory footprint. The approach is also able to handle long-term contexts with a bounded memory footprint, and it is able\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, ques in enumerate(questions):\n",
    "    res = ask_question(ques, vectorstore, model, tokenizer, terminators)\n",
    "    print('\\033[94m' + ques + '\\033[0m')\n",
    "    print('\\033[96m' + res['response'] + '\\033[0m' + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aecc769-ff58-4077-bda0-38cac1a66467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0a68a-9db8-48be-89a5-97d5dcad5462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
